---
title: "Hit Songs Through the Decades"
subtitle: "A Machine Learning Approach to Predicting Release Periods and Analyzing Trends in Popular Music"

author:

 - Andrea Lovato
 - Elwin Freudiger
 - Maya Wahart

institute: University of Lausanne
date: "May 17, 2025"
title-block-banner: "#0095C8" # chosen for the university of lausanne
toc: true
toc-location: right
format: 
  html:
    number-sections: true
    html-math-method: katex
    self-contained: true
    code-overflow: wrap
    code-fold: true
    code-tools: true
    theme: 
      light: "flatly"
      dark: "darkly"
    include-in-header: 
      text: |
        <style type="text/css">
          .quarto-title-banner a {
            color: #000000;
          }
        </style>

editor: visual

abstract: |
---

```{r}
#| echo: false
#| message: false

library(knitr)
library(tidyverse)
library(kableExtra)
library(broom)
library(reshape2)
library(magrittr)
library(corrplot)
library(plotly)
library(ggcorrplot)
library(factoextra)
library(FactoMineR)
library(dplyr)
library(ggplot2)
library(corrplot)
library(randomForest)
library(rpart)
library(rpart.plot)
library(lubridate)
library(caret)
library(rsample)
```

```{r}
#| echo: false
#| message: false
library(readr)
spotify <- read_csv("~/ML project/top_10000_1950-now.csv")
#spotify <- read_csv("top_10000_1950-now.csv")
```

# 1. Introduction

## Project Goals

The primary objective of this project is to apply machine learning techniques to a dataset of 10,000 top-charting songs from the ARIA and Billboard rankings, spanning from the 1950s to 2024. Using audio and metadata features provided by Spotify, the project aims to explore the evolution of musical trends and understand the key characteristics that define popular music across different decades.

From a machine learning perspective, the project focuses on both supervised and unsupervised learning tasks. This includes building predictive models to estimate a song’s release period based on its features, and using clustering algorithms to uncover latent patterns in the data. The goal is not only to analyze historical music trends but also to assess the feasibility and accuracy of predictive modeling in the context of music analytics.

# Literature review

## Music Analytics and Machine Learning

The intersection of music and data science has increasingly attracted scholarly attention, particularly due to the rise of streaming platforms that provide large-scale, structured musical data. Several studies have explored the potential of audio and metadata features—such as those provided by Spotify—for understanding musical trends, classifying genres, and predicting popularity (Schedl et al., 2015). These features typically include danceability, energy, tempo, and valence, which encode perceptual and structural dimensions of sound and have been shown to correlate with human musical preferences (Friberg et al., 2011).

Machine learning techniques have been applied extensively to music data for tasks such as mood classification, genre detection (Kim et al., 2010), and hit song prediction (Herremans et al., 2019). Supervised models, particularly random forests and support vector machines, have demonstrated robust performance in tasks involving feature-based classification and regression. More recent work has applied deep learning architectures to raw audio input, though these approaches often demand substantial computational resources and training data (Choi et al., 2017).

Unsupervised learning, including clustering and dimensionality reduction techniques such as PCA, has also been used to explore latent patterns in musical corpora. For example, Jan Van Balen et al. (2015) used clustering to discover prototypical musical structures, while Serrà et al. (2012) applied PCA to uncover dominant stylistic trends over time.

## Temporal Trends and Musical Evolution

Temporal analyses have shown that the characteristics of popular music evolve in response to technological, cultural, and economic forces (Mauch et al., 2015). For instance, changes in average song duration have been linked to shifts in radio programming, physical formats (e.g., vinyl, CDs), and digital streaming incentives. The increasing prevalence of explicit lyrics has been attributed to looser content restrictions and changing cultural norms (Pachet, 2008).

Empirical work by Interiano et al. (2018) used Spotify audio features to track the emotional content of popular music, observing a long-term trend toward increased sadness and decreased acousticness in top-charting songs. Similarly, it has been demonstrated that temporal audio descriptors could be used to model the release decade of songs with reasonable accuracy.

## Challenges in Modeling Popularity and Period

While popularity is a central variable in music analytics, its measurement remains opaque. Spotify’s proprietary popularity score is influenced by recent streaming activity, skips, and playlist placements, making it a dynamic and platform-specific metric (Spotify, 2023). Consequently, its use in predictive models must be interpreted as a temporal snapshot rather than a static attribute.

Predicting a song’s release year or period based on its acoustic profile poses challenges due to the high variability of musical styles within the same time frame and the enduring popularity of certain older songs. Nevertheless, several studies have demonstrated that audio features carry enough temporal signal to support classification tasks by decade or era, particularly when modeling broader stylistic shifts (Müller et al., 2010).

## Contribution of the Present Study

Building on this body of literature, the present study contributes a comprehensive analysis of 10,000 top-charting songs from the ARIA and Billboard rankings, spanning more than seven decades. While prior work has focused either on genre classification or mood prediction, this study uniquely combines exploratory data analysis, unsupervised learning (e.g., PCA, clustering), and supervised modeling to assess both the temporal evolution of music and the predictive power of audio features.

Moreover, by investigating the relationships between explicitness, duration, and popularity in a temporal context, this work highlights how platform incentives and listener behaviors have shaped recent musical trends. It also addresses gaps in the literature regarding the feasibility of using readily available Spotify features for temporal classification, and evaluates the limitations of clustering methods in capturing stylistic boundaries.

Finally, this study serves as a practical application of machine learning techniques in the music domain, offering insights for musicologists, data scientists, and digital media analysts interested in the computational modeling of cultural data.

# 2. Data

## Sources

The dataset employed in this study, titled *“Top 10,000 Spotify Songs – ARIA and Billboard Charts”*, was obtained from Kaggle, a widely used platform for sharing datasets and data science resources. It comprises a curated collection of 10,000 tracks that have achieved significant popularity, based on historical rankings from both the ARIA (Australian Recording Industry Association) and Billboard charts. This dual-source approach ensures a broad and balanced representation of commercially successful music across English-speaking markets.

The dataset spans a temporal range from the 1950s to 2024, capturing the dynamic evolution of popular music over more than seven decades. It includes metadata and audio-based features extracted via the Spotify API, such as tempo, energy, danceability, valence, and instrumentalness, which allow for detailed computational analysis.

In addition to representing a variety of genres, artists, and time periods, the dataset reflects shifting cultural and musical preferences. As such, it provides a robust foundation for both exploratory data analysis and machine learning applications, particularly those aimed at uncovering temporal trends, predicting historical context (e.g., release period), and identifying latent patterns in music characteristics.

## Image Data

In order to enhance the predictions, album covers were selected to be added as input in the neural network model. The orginal dataset uses a link provided by spotify to retrieve each image. The code below shows the process used:

```{python}
#| eval: false

import pandas as pd
import numpy as np

import requests
import os
from tqdm import tqdm
from io import BytesIO
from PIL import Image
import matplotlib.pyplot as plt
from concurrent.futures import ThreadPoolExecutor, as_completed

#function that processes one image
def process_image(row, size = (64,64), out_dir='data/clean/images'):
    id = row['id']
    link = row['Album Image URL'] 
    try:
        response = requests.get(link, timeout=10)
        image = Image.open(BytesIO(response.content)).convert("RGB")
        #resizing of the image
        final_image = image.resize(size)
        out_path = os.path.join(out_dir, f"{id}.png")
        final_image.save(out_path)
        return {'id': id, 'filepath': out_path}
    except Exception as e:
        print(f"Failed for {id}: {e}")
        return None



def main(dir='data/clean/images'):
    if not os.path.exists(dir):
        os.makedirs(dir)
    #input dataset with links
    df = pd.read_csv('data/raw/top_10000_1950-now.csv')
    df['id'] = df['Track URI'].str.slice(start=14)

    #process them in parallel by using threading to speed up the process
    results = []

    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(process_image, row, out_dir=dir): row['id'] for _, row in df.iterrows()}
        for future in as_completed(futures):
            result = future.result()
            if result:
               results.append(result)

    # save all image path
    image_df = pd.DataFrame(results)
    image_df.to_csv('data/clean/image_dataset.csv', index=False)
    print('Saved images and csv!')

if __name__ == "__main__":
    main()
    
```

As can be seen, all images are retrieved from their respective links, resized to be 64x64 pixels. The size was chosen to balance concerns of space and usefulness. Indeed, a 16x16 pixel image would provide hardly any useful information. On the other hand, a 600x600 image would provide plenty of information, but would be difficult to store. All images are then saved locally, with their Spotify Song ID as a filename. This process was performed in parallel using multi-treading. This enables a much faster processing time.

Examples images can be seen here: [Taylor Swift's Red Ablum cover](../data/clean/images/5ybJm6GczjQOgTqmJ0BomP.png) [Sabrina Carpenter's Short n' Sweet Ablum cover](../data/clean/images/0lBwoEOqZNwm07Pd8Hwj3L.png) [AC/DC's Dirty Deeds Done Dirt Cheap Ablum cover](../data/clean/images/2d4e45fmUnguxh6yqC7gNT.png) [ABBA's Arrival Ablum cover](../data/clean/images/0GjEhVFGZW8afUYGChu3Rr.png)

## Description

```{r}
dim(spotify)
```

The dataset comprises 10,000 entries and 35 variables, encompassing information related to song popularity, artist identity, release date, and various musical attributes.

| Variable | Description | Category | Example |
|------------------|------------------|------------------|------------------|
| track_uri | Unique identifier for the track | character | spotify:track:123... |
| track_name | Name of the track | character | Bohemian Rhapsody |
| artist_uris | URIs of artists performing the track | character | spotify:artist:abc... |
| artist_names | Name(s) of the performing artist(s) | character | Queen |
| album_uri | Unique identifier for the album | character | spotify:album:def... |
| album_name | Title of the album | character | A Night at the Opera |
| album_artist_uris | URIs of the album's main artist(s) | character | spotify:artist:abc... |
| album_artist_names | Name(s) of the album's main artist(s) | character | Queen |
| release_date | Date the album was released | date | 1975-11-21 |
| album_image_url | Link to album cover image | character | https://i.scdn.co/image/... |
| disc_number | Disc number of the track in multi-disc sets | numeric | 1 |
| track_number | Track's position on the disc | numeric | 11 |
| duration_ms | Length of the track in milliseconds | numeric | 354000 |
| preview_url | URL to 30-second preview of the track | character | https://p.scdn.co/mp3-preview/... |
| is_explicit | Indicates if track has explicit content | logical | TRUE |
| popularity | Spotify popularity score (0-100) | integer | 85 |
| isrc | International Standard Recording Code | character | GBUM71029604 |
| added_by | User who added the track to playlist | character | user_id_123 |
| added_at | Timestamp when track was added | datetime | 2022-07-15T12:00:00Z |
| artist_genres | Genres associated with the artist(s) | character | rock, classic rock |
| danceability | How suitable a track is for dancing | numeric | 0.6 |
| energy | Intensity and activity level of the track | numeric | 0.85 |
| key | Musical key of the track (0=C, 1=C♯, ...) | integer | 5 |
| loudness | Overall loudness in decibels | numeric | -5.3 |
| mode | Modality: major (1) or minor (0) | integer | 1 |
| speechiness | Presence of spoken words in the track | numeric | 0.05 |
| acousticness | Confidence that track is acoustic | numeric | 0.02 |
| instrumentalness | Likelihood that track is instrumental | numeric | 0.001 |
| liveness | Likelihood of live audience presence | numeric | 0.09 |
| valence | Musical positiveness conveyed | numeric | 0.7 |
| tempo | Beats per minute (BPM) | numeric | 120.5 |
| time_signature | Estimated time signature | integer | 4 |
| album_genres | Genres associated with the album | character | rock, progressive rock |
| label | Record label | character | EMI |
| copyrights | Copyright info for the album or track | character | ℗ 1975 Queen Productions Ltd. |

```{r}
#| echo: false

# Preview the dataset
knitr::kable(head(spotify), 
             format = "markdown", 
             align = 'c', 
             table.attr = 'class="table table-bordered"',
             row.names = FALSE) %>%
  kable_styling(font_size = 10, full_width = FALSE, position = "center")

```

## Wrangling / Cleaning

In preparation for analysis, the original dataset was cleaned and standardized by renaming variables to follow consistent, machine-readable naming conventions. Redundant or non-essential columns—such as URIs, preview links, and metadata unrelated to audio features—were subsequently removed. The resulting dataset *spotify_vr* retains only the relevant musical, temporal, and popularity-related attributes needed for the subsequent exploratory and predictive modeling tasks.

```{r}
spotify <- spotify |>
  rename(
    track_uri            = `Track URI`,
    track_name           = `Track Name`,
    artist_uris          = `Artist URI(s)`,
    artist_names         = `Artist Name(s)`,
    album_uri            = `Album URI`,
    album_name           = `Album Name`,
    album_artist_uris    = `Album Artist URI(s)`,
    album_artist_names   = `Album Artist Name(s)`,
    release_date         = `Album Release Date`,
    album_image_url      = `Album Image URL`,
    disc_number          = `Disc Number`,
    track_number         = `Track Number`,
    duration_ms          = `Track Duration (ms)`,
    preview_url          = `Track Preview URL`,
    is_explicit          = `Explicit`,
    popularity           = `Popularity`,
    isrc                 = `ISRC`,
    added_by             = `Added By`,
    added_at             = `Added At`,
    artist_genres        = `Artist Genres`,
    danceability         = `Danceability`,
    energy               = `Energy`,
    key                  = `Key`,
    loudness             = `Loudness`,
    mode                 = `Mode`,
    speechiness          = `Speechiness`,
    acousticness         = `Acousticness`,
    instrumentalness     = `Instrumentalness`,
    liveness             = `Liveness`,
    valence              = `Valence`,
    tempo                = `Tempo`,
    time_signature       = `Time Signature`,
    album_genres         = `Album Genres`,
    label                = `Label`,
    copyrights           = `Copyrights`
  )
```

```{r}
spotify_vr <- spotify |>
  select(-artist_uris, -album_uri,-album_artist_uris,-album_artist_names, -disc_number,-preview_url, -isrc, -added_by, -added_at, -album_genres, -copyrights,-key)

ncol(spotify_vr)
```

The resulting dataset consists of 23 columns, containing only the variables relevant for the subsequent analysis after the removal of redundant and non-informative features.

We only keep the ID from the Spotify track URL by extracting the final component of the *track_uri* string.

```{r}
library(dplyr)
library(stringr)

spotify_vr <- spotify_vr |>
  mutate(track_uri = str_extract(track_uri, "[^:]+$"))
```

```{r}
#| echo: false

# Preview the dataset
knitr::kable(head(spotify_vr), 
             format = "markdown", 
             align = 'c', 
             table.attr = 'class="table table-bordered"',
             row.names = FALSE) %>%
  kable_styling(font_size = 10, full_width = FALSE, position = "center")
```

## Spotting Mistakes and Missing Data

The procedure involves identifying and counting missing values in the dataset, detecting rows containing incomplete information, and removing those with missing or empty loudness values. Then, the release year is extracted from the release date and converted to a numeric format, producing a cleaned dataset ready for further analysis.

The dataset have a total of 625 missing values before cleaning. After cleaning we still have 547 missing values, all of them concern the *artist_genres*.

```{r}
#| echo: false
#| message: false

# Count total missing values
sum(is.na(spotify_vr))  # This will return the total number of NAs

dataset_clean <- spotify_vr |>
  filter(!is.na(loudness) & loudness != "") %>% 
  mutate(release_year = as.numeric(substr(release_date, start = 1, stop = 4)))

#Drop instance with missing release date
dataset_clean <- dataset_clean |>
  filter(release_date != "0000")

colSums(is.na(dataset_clean)) |>
  head(20) |>  # Limit to first 20 rows for readability
  kable(
    format = "html",
    caption = "Rows with Missing or Blank Values"
  ) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE
  )
```

The dataset contains a total of 625 missing values, of which 551 correspond to the *artist_genres* variable. Instances with missing values in variables related to musical characteristics are excluded from the analysis to ensure data quality and consistency.

## Listing Anomalies and Outliers

A subset of the dataset containing only numeric variables was selected, excluding *mode* and *time_signature*. The data was then transformed into long format to generate faceted boxplots, allowing for the visualization of the distribution and scale of each numeric variable individually.

```{r}
numeric_data <- dataset_clean %>% select(where(is.numeric)) |>
  select(-mode,-time_signature)

long_data <- pivot_longer(numeric_data, cols = everything(), names_to = "Variable", values_to = "Value")

# Faceted boxplots with individual y-axis scales
ggplot(long_data, aes(x = "", y = Value)) +
  geom_boxplot() +
  facet_wrap(~ Variable, scales = "free_y") +
  labs(title = "Boxplots for Numeric Variables (Individual Scales)", x = "", y = "") +
  theme_minimal()
```

### Interpretation

The faceted boxplots illustrate the distribution of key numeric features. Variables such as acousticness, energy, danceability, instrumentalness, liveness, valence, and speechiness are bounded between 0 and 1. Most exhibit distributions concentrated near zero, particularly instrumentalness and speechiness, which show long right tails and outliers close to 1, suggesting that while the average track lacks strong instrumental or spoken elements, some are highly characterized by them.

Duration (in milliseconds) is slightly right-skewed, with most tracks clustered around the median and a few outliers representing exceptionally long songs. Loudness is centered around negative values, consistent with its decibel scale relative to silence, and shows a compact distribution with occasional extreme lows, likely corresponding to quiet or highly dynamic tracks.

Popularity displays a broad distribution with outliers at both ends, indicating notable variability in audience reception. Tempo shows high variance and several extreme values, potentially due to anomalous entries or experimental compositions. Lastly, track number is typically low—reflecting songs positioned early in albums—though outliers suggest the presence of long compilations or inconsistent metadata.

## Correlation Matrix

# 3. Exploratory Data Analysis

To gain a comprehensive understanding of the dataset and uncover patterns relevant to the modeling phase, an exploratory data analysis (EDA) is conducted. This phase involves the systematic examination of the dataset’s structure, distributions, and relationships among variables. We begin with summary statistics and visual inspections of key numeric features, followed by the analysis of correlations and potential multicollinearity. Subsequently, we investigate temporal trends, track characteristics, and the distribution of categorical variables such as explicit content and genre. This stepwise exploration helps identify data quality issues, potential outliers, and underlying trends that may influence or inform the subsequent application of machine learning techniques.

## Table description

????

For this data analysis, we will see different graphs....

## Summary statistics

The summary statistics table provides an overview of the central tendencies and dispersion of key variables in the dataset.

As we can see, regarding the release date, most songs have been released in more recent years. this may lead to unbalanced data. Ways to solve this unbalance will be discussed later in this paper. Regarding the track number in the song, the mean being at *4.9*, this tells us that most songs may part of an album or compilation. interestingly, the max of song number is *93* for a song named *Soul Revival* by *Johnny Diesel & The Injectors* part of a *Complete Eighties* compilation of 100 songs from the 80s.

The track duration is expressed in milliseconds, with a mean of *3 minutes and 44 seconds.* The longest song in record is a whopping 26 minutes. for *Tubular Bells - Pt. I* by *Mike Oldfield*. While this may not ring a bell (pun intended) for most readers, Amateurs of Horror may recognize this as the opening soundtrack for *The Exorcist (1973)* The audio preview below:

```{=html}
<audio controls>
  <source src="https://p.scdn.co/mp3-preview/2da01a5ad77fddb1bc2b0ed9e138a74eabe43333?cid=9950ac751e34487dbbe027c4fd7f8e99" type="audio/mpeg">
  Your browser does not support the audio element.
</audio>
```

The is_explicit variable is highly imbalanced, with approximately 95% of the songs labeled as non-explicit. The distribution of the popularity variable is not normally distributed, with a mean value of 33, indicating that most songs fall within a lower popularity range. Variables such as danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, and time signature are audio features provided by Spotify that describe various musical characteristics of each track.

```{r}
dataset_clean %>%
  summary() %>%
  kable() %>%
  kable_styling()
```

## Correlation matrix

A correlation matrix is computed and visualized using a color-coded upper-triangle plot to identify linear relationships among the numeric variables in the cleaned dataset. This helps reveal patterns of association and potential multicollinearity between audio features.

The correlation matrix reveals a moderate positive association between energy and loudness, suggesting that more energetic tracks tend to be louder. A notable negative correlation is observed between acousticness and energy, indicating that acoustic songs generally exhibit lower energy levels. Overall, the absence of strong correlations among most variables suggests low multicollinearity, supporting their joint inclusion in multivariate analyses.

```{r}
#start by selecting numeric columns
num_dataset <- dataset_clean %>% 
  select_if(is.numeric)

cor_matrix <- cor(num_dataset, use = "complete.obs")

ggcorrplot(
  cor_matrix,
  method    = "square",   
  lab       = TRUE,       
  lab_size  = 2,           
  tl.cex    = 10,        
  colors    = c("blue", "white", "red"), 
  outline.col = "gray80",
) +
  labs(
    title = "Correlation Matrix for Song features"
  ) +
  xlab(NULL)+ ylab(NULL)+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))

```

## Variables analysis

### Release year

```{r}
#adding a histogram with songs by year
year_hist <- dataset_clean %>% 
  select(release_year) %>%
  filter(release_year>0) %>%
  ggplot(aes(x=release_year)) +
  geom_histogram(binwidth = 1, fill="blue", color="black") +
  theme_minimal() +
  labs(title = "Number of songs by release year", x="Year") 
year_hist
#year_hist %>% ggplotly()
```

Release year is the dependent variable in this analysis, and it is considered at the level of yearly granularity. The histogram shows a strong concentration of songs in more recent years, indicating a temporal imbalance that may affect the representativeness of earlier decades in the modeling phase.

### Track numbers

```{r}
track_dens <- dataset_clean %>% 
  filter(track_number!=0) %>% 
  ggplot(aes(x=track_number)) +
  geom_histogram(fill='blue', color='black') + 
  theme_minimal() +
  labs(title = "Track number density", y="Density", x="track number")
track_dens
  
```

The distribution of track numbers is highly skewed toward lower values, indicating that most songs appear early in albums, while higher values likely reflect compilations or large tracklists.

It may be interesting to assess whether Benford's Law applies to the distribution of track numbers.

```{r}
benford_distr <- data.frame(
  first_digit = 1:9,
  benford_pct = log10(1 + 1/(1:9)) * 100
)

benford <- dataset_clean %>% 
  filter(track_number!=0) %>% 
  mutate(first_digit = as.numeric(substr(as.character(track_number), 1, 1))) %>% 
  ggplot(aes(x = first_digit)) +
  geom_histogram(
    bins = 9,
    fill = "blue",
    color = "black",
    aes(y = after_stat(count / sum(count) * 100))
  ) +
  geom_line(
    data = benford_distr,
    aes(x = first_digit, y = benford_pct),
    color = "red",
    size = 1
  ) +
  scale_x_continuous(breaks = 1:9) +
  labs(
    title = "First Digit Distribution of Track Numbers vs Benford's Law",
    x = "First Digit",
    y = "Percentage"
  ) +
  theme_minimal()

benford

```

The distribution of the first digit in track numbers clearly deviates from the expected pattern described by Benford’s Law. This discrepancy is likely due to structural constraints in how music albums are organized. Most releases—such as singles, EPs, and standard albums—contain a relatively small and fixed number of tracks, typically ranging from 1 to 15. As a result, lower digits, particularly 1, dominate the distribution, not because of a naturally logarithmic phenomenon but due to intentional sequencing and formatting practices in album production. This illustrates how domain-specific conventions can override general statistical laws in structured datasets.

Lastly, the evolution of track numbers over time can be examined to assess whether album structure or track positioning has changed across decades, potentially reflecting shifts in music consumption formats or production practices.

```{r}
year_tracknum <- dataset_clean %>% 
  filter(release_year!=0,
         track_number!=0) %>% 
  ggplot(aes(x=release_year, y=track_number)) +
  geom_point(color = "blue", size = 2) +
  theme_minimal()  + 
  labs(title="Track number depending on the release year", y="Track Number", x="Release Year")
year_tracknum
```

The scatter plot shows no clear trend in the evolution of track numbers over time. While the majority of songs consistently appear in early album positions across decades, some outliers—particularly in more recent years—exceed typical album lengths, likely reflecting special editions, compilations, or digital releases with extended tracklists.

### Track Duration

The distribution of track duration, expressed in milliseconds, is examined to determine typical song lengths and to identify potential outliers, including exceptionally short or long tracks, which may affect subsequent analyses.

```{r}
duration_hist <- dataset_clean %>% 
  drop_na() %>% 
  mutate(duration_ms = duration_ms/1000) %>% 
  ggplot(aes(x=duration_ms)) +
  geom_histogram(bins=50, fill='blue', color='black') +
  theme_minimal() +
  labs(title ="Histogram of track durations", x="Track Duration (s)")
duration_hist
```

The histogram shows that the distribution of track durations is right-skewed, with most songs clustered around typical lengths and a limited number of extreme values representing unusually long tracks.

A line plot is used to visualize the evolution of median track duration over time. This allows identification of long-term trends, such as shifts in typical song lengths across decades, and highlights notable changes in production or consumption patterns.

```{r}
median_year <- dataset_clean %>% 
  drop_na() %>% 
  group_by(release_year) %>% 
  summarise(median_duration = median(duration_ms, na.rm = TRUE)) %>% 
  mutate(median_duration = median_duration/1000) %>% 
  ggplot(aes(y=median_duration, x=release_year))+
  geom_line(color='blue', size=1) + 
  theme_minimal()+
  labs(title="Evolution of median track duration by year", y="Median Duration (s)", x="Year")
median_year
```

The plot shows that median track duration increased steadily throughout the second half of the 20th century, reaching a peak in 1992 at approximately 260 seconds (4 minutes and 20 seconds). A gradual decline follows, particularly from the 2010s onward. This recent downward trend is likely influenced by changes in digital consumption patterns, where streaming platforms incentivize shorter songs due to skip behavior and payout structures based on play counts.

```{r}
duration_pop <- dataset_clean %>% 
  drop_na() %>%
  mutate(duration_ms = duration_ms/1000) %>% 
  ggplot(aes(y=duration_ms, x=popularity)) +
  geom_point(color="blue")
duration_pop
```

The scatter plot shows no strong correlation between duration and popularity, though popular songs tend to have standard lengths, while very long tracks are generally less popular.

### is_explicit

The proportion of explicit versus non-explicit songs is analyzed over time to observe how the prevalence of explicit content has evolved across different release years.

```{r}
explicit_year <- dataset_clean %>% 
  drop_na() %>% 
  group_by(release_year, is_explicit) %>%
  summarise(count = n(), .groups = "drop") %>% 
  ggplot(aes(x = release_year, y = count, fill = as.factor(is_explicit))) +
  geom_area(position = "fill", alpha=0.6) +
  scale_fill_manual(
    values = c("TRUE" = "red", "FALSE" = "lightgray"),
    name = "Explicit",
    labels = c("FALSE" = "Non-Explicit", "TRUE" = "Explicit")
  ) +
  labs(
    title = "Evolution of Explicit vs. Non-Explicit Songs",
    x = "Year",
    y = "Share",
    fill = "Explicit"
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()

explicit_year
```

The analysis reveals a clear upward trend in the share of explicit songs over time, with a marked increase beginning in the early 2000s. This suggests a shift in lyrical content or labeling practices in the streaming era, where explicit content has become more common in popular music releases.

A line plot is now used to visualize the annual count of explicit songs over time, offering a more detailed view of their increasing presence in recent decades.

```{r}
line_explicit <- dataset_clean %>% 
  drop_na() %>% 
  group_by(release_year) %>%
  summarise(
    total = n(),
    explicit_count = sum(is_explicit, na.rm = TRUE),
    explicit_share = explicit_count / total
  ) %>% 
  ggplot(aes(x = release_year, y = explicit_count)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Share of Explicit Songs Over Time",
    x = "Year",
    y = "Share of Explicit Songs"
  ) +
  theme_minimal()
line_explicit
```

The line plot confirms a rising trend in the number of explicit songs over time, particularly from the early 2000s onward. However, this pattern should be interpreted cautiously, as the explicit label is assigned by content uploaders and may not consistently reflect the presence of explicit material. In some cases, tracks with potentially explicit content are also released in censored versions to ensure broader distribution, such as radio play.

A boxplot is used to compare the distribution of song popularity between explicit and non-explicit tracks, allowing for the assessment of whether explicit content is associated with higher or lower popularity levels.

```{r}
explicit_boxplot <- dataset_clean %>% 
  drop_na() %>% 
  ggplot(aes(x = as.factor(is_explicit), y = popularity)) +
  geom_boxplot() +
  labs(
    title = "Popularity by Explicit Status",
    x = "Explicit",
    y = "Popularity"
  ) +
  theme_minimal()
explicit_boxplot
```

To further explore the relationship between explicit content and popularity, a scatter plot is used to visualize popularity scores over time, differentiated by explicit status. This approach helps assess whether the observed higher popularity of explicit songs is inherently tied to their content or instead influenced by temporal release patterns.

```{r}
explicit_popdate <- dataset_clean %>% 
  drop_na() %>% 
  ggplot(aes(x = release_year, y = popularity, color=as.factor(is_explicit))) +
  scale_color_manual(
    values = c("TRUE" = "red", "FALSE" = "blue"),
    name = "Explicit",
    labels = c("FALSE" = "Non-Explicit", "TRUE" = "Explicit")
  ) +
  geom_point() +
  labs(
    title = "Popularity by year with explicit status",
    x = "Release Year",
    y = "Popularity"
  ) +
  theme_minimal()
   #+ facet_wrap(vars(is_explicit))
explicit_popdate
```

The scatter plot reveals that explicit songs (in red) are predominantly concentrated in more recent years and often exhibit high popularity scores. This supports the hypothesis that the observed popularity advantage of explicit tracks may be confounded by release date, as newer songs are both more likely to be explicit and to benefit from recency effects in popularity metrics.

### Popularity

Popularity serves as a valuable proxy for a song’s commercial success and audience reach. While the exact algorithm used to calculate Spotify’s popularity score is not publicly disclosed, it is reasonable to assume that it reflects streaming frequency over a defined recent time window.

```{r}
# Summary statistics for key numeric features
dataset_clean %>%
  select(track_name, artist_names, popularity) %>% 
  arrange(desc(popularity)) %>%
  head() %>% 
  kable() %>%
  kable_styling()

```

It is important to note that the dataset was last updated in October 2024, meaning that the popularity scores reflect a specific point in time and may have since changed. However, this temporal limitation does not affect the validity of the present analysis, as the values still offer a reliable snapshot for exploring general trends and patterns in music popularity.

```{r}
avg_pop_date <- dataset_clean %>% 
  drop_na() %>% 
  group_by(release_year) %>% 
  summarise(avg_pop = mean(popularity, na.rm = TRUE)) %>%
  ggplot(aes(y=avg_pop, x=release_year))+
  geom_line(color='blue', size=1) + 
  theme_minimal()+
  labs(title="Evolution of mean popularity by year", y="Popularity", x="Year")
avg_pop_date
```

The chart shows peaks in popularity for early and recent songs. Older tracks remain culturally relevant, while the recent rise reflects the streaming era’s impact on visibility and access.

While the high popularity of recent songs is expected, the elevated scores for tracks from the 1960s are more surprising. A possible explanation is that many of these songs—such as those by The Beatles—have maintained lasting cultural relevance and continue to be featured in playlists and rankings, preserving their popularity across generations.

## Text mining

This section performs a basic text mining task by extracting and counting individual artist names from multi-artist tracks. The goal is to identify the top 30 most frequently appearing artists in the dataset.

```{r}
# Separate multiple artists
top_artists <- dataset_clean %>%
  filter(!is.na(artist_names)) %>%
  mutate(artist_names = str_split(artist_names, ",")) %>%
  unnest(artist_names) %>%
  mutate(artist_names = str_trim(artist_names)) %>%
  count(artist_names, sort = TRUE) %>%
  slice_head(n = 30)

# Plot
artist_bar <- ggplot(top_artists, aes(x = reorder(artist_names, n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 30 Artists by Song Count",
    x = "Artist",
    y = "Number of Songs"
  ) +
  theme_minimal()
artist_bar
```

This step identifies the top 10 artists with the highest average popularity scores, offering insights into which artists consistently produce well-received songs within the dataset.

```{r}
artist_pop <- dataset_clean %>%
  filter(!is.na(artist_names)) %>%
  mutate(artist_names = str_split(artist_names, ",")) %>%
  unnest(artist_names) %>%
  mutate(artist_names = str_trim(artist_names)) %>%
  group_by(artist_names) %>% 
  summarise(avg_pop = mean(popularity, na.rm = TRUE)) %>%
  arrange(desc(avg_pop)) %>%
  slice_head(n=10) %>% 
  kable() %>%
  kable_styling()
artist_pop
```

The following analysis ranks artists by their average track duration, highlighting those associated with longer musical compositions. Such patterns may reflect stylistic tendencies common in certain genres, including progressive rock, ambient, or experimental music.

```{r}
artist_len <- dataset_clean %>%
  filter(!is.na(artist_names)) %>%
  mutate(artist_names = str_split(artist_names, ",")) %>%
  unnest(artist_names) %>%
  mutate(artist_names = str_trim(artist_names)) %>%
  group_by(artist_names) %>% 
  summarise(avg_time = mean(duration_ms, na.rm = TRUE)) %>%
  arrange(desc(avg_time)) %>%
  slice_head(n=10) %>% 
  kable() %>%
  kable_styling()
artist_len
```

The following analysis focuses on the artists with the lowest average track durations, aiming to reveal trends related to brevity in musical production, which may be influenced by genre conventions or platform-driven listening behaviors.

```{r}
artist_len_inc <- dataset_clean %>%
  filter(!is.na(artist_names)) %>%
  mutate(artist_names = str_split(artist_names, ",")) %>%
  unnest(artist_names) %>%
  mutate(artist_names = str_trim(artist_names)) %>%
  group_by(artist_names) %>% 
  summarise(avg_time = mean(duration_ms, na.rm = TRUE)) %>%
  arrange(avg_time) %>%
  slice_head(n=10) %>% 
  kable() %>%
  kable_styling()
artist_len_inc
```

A similar type of analysis is now performed based on musical genre, in order to examine how average song characteristics vary across different stylistic categories.

```{r}
top_genres <- dataset_clean %>%
  mutate(artist_genres = replace_na(artist_genres, "No genre")) %>% 
  filter(!is.na(artist_genres)) %>%
  mutate(artist_genres = str_split(artist_genres, ",")) %>%
  unnest(artist_genres) %>%
  mutate(artist_genres = str_trim(artist_genres)) %>%
  count(artist_genres, sort = TRUE) %>%
  slice_head(n = 20)

# Plot
genre_bar <- ggplot(top_genres, aes(x = reorder(artist_genres, n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 20 Genres by Song Count",
    x = "Genre",
    y = "Number of Songs"
  ) +
  theme_minimal()
genre_bar
```

```{r}
genre_pop <- dataset_clean %>%
  mutate(artist_genres = replace_na(artist_genres, "No genre")) %>% 
  filter(!is.na(artist_genres)) %>%
  mutate(artist_genres = str_split(artist_genres, ",")) %>%
  unnest(artist_genres) %>%
  mutate(artist_genres = str_trim(artist_genres)) %>%
  group_by(artist_genres) %>% 
  summarise(avg_pop= mean(popularity, na.rm = TRUE)) %>%
  arrange(desc(avg_pop)) %>%
  slice_head(n=10) %>% 
  kable() %>%
  kable_styling()
  
genre_pop
```

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ to be continued:

# 4. Machine Learning Methods

# 5. Results

## Unsupervised Learning

### Clustering

```{r}
clustering_data <- dataset_clean %>% 
  select(c(where(is.numeric), is_explicit)) %>% 
  select(-c(release_year)) %>% 
  drop_na() %>% 
  scale()
```

### K-means clustering

We'll start by fitting a k-means clustering. First, we need to determine the optimal number of clusters to use.

```{r}
fviz_nbclust(clustering_data,
             kmeans,
             method='wss',
             k.max = 100,
             verbose = FALSE)

```

As can be seen, the elbow seems to converge at around 5 clusters. We will therefore, take this as our number of clusters.

```{r}
song_km <- kmeans(clustering_data, centers=30)
```

To inspect these clusters we will start by having boxplots

```{r}
song_comp <- data.frame(clustering_data, 
                        Clust=factor(song_km$cluster),
                        Id=row.names(dataset_clean))
song_df <- melt(song_comp, id=c("Id", "Clust"))
head(song_df, 10)

ggplot(song_df, aes(y=value, group=Clust, fill=Clust)) +
  geom_boxplot() +
  facet_wrap(~variable)
```

As can be seen, no pattern seems to emerge from the k-means clustering. We may still attempt to have hierarchical clustering.

### Hierarchical clustering

```{r}
song_distances <- dist(clustering_data, method = "manhattan")
song_melt <- melt(as.matrix(song_distances)) 
head(song_melt)
```

```{r}
song_hc <- hclust(song_distances, method="complete")
```

```{r}
plot(song_hc, hang=-1)
rect.hclust(song_hc, k=13)
song_clust_hc <- cutree(song_hc, k=13)
song_clust_hc
```

Hierarchical clustering does not seem to accurately show differences between songs. Many reasons may lead to it being this way.

### PCA

```{r}
# 1. Prepare numeric data and keep release_year for color
pca_data <- dataset_clean %>%
  filter(release_year>0) %>% 
  drop_na() %>% 
  select(c(danceability, energy, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo)) %>%
  scale()

# 2. Run PCA on everything except release_year
pca_result <- prcomp(pca_data, scale. = TRUE)

# 3. Extract PCA scores and add release year for coloring
pca_scores <- as.data.frame(pca_result$x[, 1:2])
release_years <- dataset_clean %>%
  filter(release_year > 0) %>%
  drop_na() %>%
  pull(release_year)
pca_scores$release_year <- release_years

# 4. Extract loading for arrows
loadings <- as.data.frame(pca_result$rotation[, 1:2])
loadings$variable <- rownames(loadings)

# 5. Plot PCA with arrows and color by release year
ggplot(pca_scores, aes(x = PC1, y = PC2, color = release_year)) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c(option = "plasma") +
  geom_segment(data = loadings,
               aes(x = 0, y = 0, xend = PC1 * 5, yend = PC2 * 5),  # arrows, scaled
               arrow = arrow(length = unit(0.2, "cm")), color = "gray30") +
  geom_text(data = loadings,
            aes(x = PC1 * 5.3, y = PC2 * 5.3, label = variable),
            size = 3, color = "gray20") +
  labs(
    title = "PCA Biplot of Song Features",
    x = "PC1",
    y = "PC2",
    color = "Release Year"
  ) +
  theme_minimal()
```

```{r}
pca_data <- dataset_clean %>%
  filter(release_year>0) %>% 
  drop_na() %>% 
  select(c(danceability, energy, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo)) %>%
  scale()

songs_pca <- PCA(pca_data, ncp = 11, graph = FALSE)
songs_pca
```

```{r}
fviz_pca_var(songs_pca)
```

```{r}
fviz_contrib(songs_pca, choice = "var", axes = 1)
```

```{r}
fviz_pca_biplot(songs_pca)
```

```{r}
fviz_eig(songs_pca, addlabels = TRUE, ncp=11)
```

```{r}
library(gridExtra)
p1 <- fviz_pca_biplot(songs_pca, axes = 1:2) 
p2 <- fviz_pca_biplot(songs_pca, axes = 3:4) 
p3 <- fviz_pca_biplot(songs_pca, axes = 5:6) 
grid.arrange(p1, p2, p3, nrow = 2, ncol=2)
```

## Supervised Learning

### Random Forest Balanced

### Regression

### Random Forest with undersampling

```{r}
spotify_year <- dataset_clean |>
  mutate(
    year = as.numeric(substr(release_date, 1, 4)),
    decade = floor(year / 10) * 10
  )

spotify_dec  <- spotify_year |>
  count(decade, sort = TRUE)
```

```{r}
spotify_cleaned <- spotify_dec %>%
  filter(decade != 0)

spotify_grouped <- spotify_cleaned %>%
  mutate(
    decade_group = case_when(
      decade %in% c(1950, 1960) ~ "1950–1960",
      TRUE ~ as.character(decade)
    )
  )

spotify_grouped %>%
  count(decade_group, sort = TRUE)
```

```{r}
# Trouver la taille du plus petit groupe
min_n <- spotify_grouped |>
  count(decade_group) |>
  summarise(min_n = min(n)) |>
  pull(min_n)

# Échantillonnage équilibré
spotify_balanced <- spotify_grouped |>
  group_by(decade_group) |>
  slice_sample(n = min_n) |>
  ungroup()

spotify_balanced %>%
  count(decade_group, sort = TRUE)
```

```{r}
features <- c("danceability", "energy", "acousticness", "instrumentalness",
              "liveness", "loudness", "speechiness", "tempo", "valence", "duration_ms")

df <- spotify_balanced |>
  select(all_of(features), decade_group) |>
  rename(decade = decade_group) |>
  mutate(decade = as.factor(decade)) |>
  na.omit()

split <- initial_split(df, prop = 0.8, strata = decade)
train <- training(split)
test <- testing(split)

# Fit model
rf_model <- randomForest(decade ~ ., data = train)

# Predict
pred <- predict(rf_model, newdata = test)

# Accuracy
mean(pred == test$decade)
```

```{r}
# Predictions
pred <- predict(rf_model, newdata = test)

# Accuracy
accuracy <- mean(pred == test$decade)
print(accuracy)

# Confusion Matrix
confusionMatrix(pred, test$decade)
```

**Accuracy** (0.410): correctly predict the decade 41% of the time

**Kappa** (0.312): Fair agreement beyond chance, barely acceptable

**P-value** (\< 2e-16): Statistically better than random guessing

**No Info Rate** (14.3%): Baseline accuracy if you predicted the majority

### Random Forest on median year

```{r}
spotify_binary <- spotify_year |>
  mutate(
    after_median = if_else(year > median(year, na.rm = TRUE), "after", "before"),
    after_median = as.factor(after_median)
  )
```

```{r}
spotify_clean <- dataset_clean |>
  mutate(year = as.numeric(substr(release_date, 1, 4)))

# Calculate and display the median
median_year <- median(spotify_dec$year, na.rm = TRUE)
print(median_year)

spotify_binary <- spotify_clean |>
  mutate(
    after_median = if_else(year > median(year, na.rm = TRUE), "after", "before"),
    after_median = as.factor(after_median)
  )
```

The median song was release in 2007. We are going to perform a random forest on wether the song was release before or after this year.

```{r}
features <- c("danceability", "energy", "acousticness", "instrumentalness",
              "liveness", "loudness", "speechiness", "tempo", "valence", "duration_ms")

df <- spotify_binary |>
  select(all_of(features), after_median) |>
  na.omit()
```

```{r}
set.seed(123)
train_idx <- sample(seq_len(nrow(df)), size = 0.8 * nrow(df))
train <- df[train_idx, ]
test <- df[-train_idx, ]


rf_model <- randomForest(after_median ~ ., data = train, ntree = 500)
```

```{r}
# Predictions
pred <- predict(rf_model, newdata = test)

# Accuracy
accuracy <- mean(pred == test$after_median)
print(accuracy)

# Confusion Matrix
library(caret)
confusionMatrix(pred, test$after_median)
```

**Accuracy** (0.764): correctly classify \~76% of the songs.

**Kappa** (0.525): moderate agreement beyond chance, pretty good.

**Balanced** Acc. (0.762): handles class imbalance, very healthy.

**Sensitivity** (0.724): correctly detect 72.4% of songs after 2007.

**Specificity** (0.800): correctly detect 80% of songs before 2007.

### Neural Networks

While Random Forest models showed satisfying results when it came to classification of track decades, attempts involving regression tasks to predict the year saw issues. It was therefore decided to attempt to fit a neural network to predict the release year using tabular data as well as album covers.

#### Model Building

The model was built and fitted using python and the **Keras** library, allowing to easily build neural networks.

The decision was made to train both inputs, the image data in a CNN and the tabular data in a DNN. the layers were then merged. into one hidden layer thereby taking in all the information from both layers and finally making a prediction.

The model summary can be seen below: ![Neural network Summary](../figures/Concat_model_spot.png) The code used to build the model can be seen below.

```{python}
#| eval: false

# load libraries
import numpy as np
import pandas as pd
import os
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras import layers, Model, utils
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt

# Load CSVs
df_images = pd.read_csv('data/clean/image_dataset.csv')
tabular_df = pd.read_csv('data/clean/tabular_data.csv')
df_images['filepath'] = df_images["filepath"].str.replace('/', "\\", regex=False)

# Merge on ID
merged_df = df_images.merge(tabular_df, on='id', how='inner')

# Keep numeric features, is_explicit is boolean but represented as numeric
numeric_cols = [
    "track_number", "duration_ms", "popularity", "danceability", "energy", "loudness", 
    "speechiness", "acousticness", "instrumentalness", "liveness", "valence", "tempo", 
    "time_signature", "is_explicit"
]
X_tab = merged_df[numeric_cols].values
y = merged_df["release_year"].values

#normalization of the release year
y_min, y_max = np.min(y), np.max(y)
y_norm = (y - y_min) / (y_max - y_min)

# Scale tabular input
scaler = StandardScaler()
X_tab_scaled = scaler.fit_transform(X_tab)

# Load and resize images
def load_image(filepath):
    img = Image.open(filepath).convert("RGB").resize((64, 64))
    return np.array(img) / 255.0

X_img = np.stack([load_image(fp) for fp in merged_df['filepath']])

# Train/test split
X_img_train, X_img_test, X_tab_train, X_tab_test, y_train, y_test = train_test_split(
    X_img, X_tab_scaled, y_norm, test_size=0.2, random_state=42
)

# CNN branch
img_input = layers.Input(shape=(64, 64, 3), name="img_input")
x = layers.Conv2D(32, (3, 3), activation="relu")(img_input)
x = layers.MaxPooling2D()(x)
x = layers.Conv2D(64, (3, 3), activation="relu")(x)
x = layers.MaxPooling2D()(x)
x = layers.Flatten()(x)
x = layers.Dense(64, activation="relu")(x)
x = layers.Dropout(0.5)(x)

# Tabular branch
tab_input = layers.Input(shape=(X_tab.shape[1],), name="tab_input")
t = layers.Dense(64, activation="relu")(tab_input)
t = layers.Dense(32, activation="relu")(t)

# Merge
combined = layers.concatenate([x, t])
z = layers.Dense(64, activation="relu")(combined)
z = layers.Dense(1, activation="sigmoid")(z) 

# Model
model = Model(inputs=[img_input, tab_input], outputs=z)

model.summary()
model.save("figures/model.keras")

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Train
history = model.fit(
    {"img_input": X_img_train, "tab_input": X_tab_train},
    y_train,
    validation_split=0.2,
    epochs=20,
    batch_size=32
)

# Evaluate on normalized scale
loss, mae = model.evaluate(
    {"img_input": X_img_test, "tab_input": X_tab_test},
    y_test
)
print(f"[Normalized] Test Loss: {loss:.4f}, MAE: {mae:.4f}")
```

As can be seen, the release year was normalized, knowing that the release years were located between 1955 and 2024, a normalization will help put every release year between 0 and 1. Therefore enabling predictions bounded between these maximum and minimums. This is also done with the use of a sigmoid activation function, making sure that forecasts are always bounded and that the model will not predict impossible years such as 2026 for example.

Results

The model was trained with 20 epochs and a batch size of 32 returned the following metrics:

**Training MSE: 0.0274** **Training MAE: 0.1133**

**Test MSE: 0.0270** **Test MAE; 0.1139**

**Test R² score: 0.4539**

These results show no sign of overfitting, regarding the metrics themselves, one should keep in mind that these metrics are for normalized years. Therefore, a mean absolute error of 0.1139 over the 69(2024-1955) years of study means that the model is on average off by 7.8 years. The R-squared score shows that while the model may capture some of the variance, a big part is due to other factors.

# 6. Conclusion & discussion

# 7. Appendix

# 8. References

Beesa, P., Naregavi, V., Imandar, J., & Thatte, S (2023). *Songs popularity analysis using Spotify data: An exploratory study*. In Vidhyayana – An International Multidisciplinary Peer-Reviewed E-Journal, Volume 8, Special Issue 7, 4th National Student Research Conference on “Innovative Ideas and Invention in Computer Science & IT with its Sustainability” (pp. 211–223). https://www.researchgate.net/publication/384286217

Choi, K., Fazekas, G., Sandler, M., & Cho, K. (2017). *Convolutional recurrent neural networks for music classification*. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2392–2396. https://doi.org/10.1109/ICASSP.2017.7952585

Friberg, A., Erkut, C., & Bresin, R. (2011). *Perceptual ratings of musical parameters*. Proceedings of the International Conference on Music Perception and Cognition (ICMPC), 193–198.

Gómez-Cañón, J. S., Cano, E., Eerola, T., Herrera, P., Hu, X., Yang, Y.-H., & Gómez, E. (2021). Music emotion recognition: Toward new, robust standards in personalized and context-sensitive applications. *EEE Signal Processing Magazine, 38*(6), 106–114.

Herremans, D., Martens, D., & Sörensen, K. (2019). *Dance hit song prediction*. University of Antwerp Operations Research Group Applied Data Mining Research Group, University of Antwerp.

Interiano, M., Kazemi, K., Wang, L., Yang, J., & Komarova, N. L. (2018). *Musical trends and predictability of success in contemporary songs in and out of the top charts*. Royal Society Open Science, 5(5), 171274. https://doi.org/10.1098/rsos.171274

Jan Van Balen, L., Burgoyne, J. A., Bountouridis, D., Müllensiefen, D., & Honingh, A. (2015). *Corpus analysis tools for computational hook discovery*. Proceedings of the 17th ISMIR Conference, 227–233.

Kim, Y. E., Schmidt, E. M., Migneco, R., Morton, B. G., Richardson, P., Scott, J., ... & Turnbull, D. (2010). *Music emotion recognition: A state of the art review*. Proceedings of the 11th International Society for Music Information Retrieval Conference (ISMIR), 255–266.

Mauch, M., MacCallum, R. M., Levy, M., & Leroi, A. M. (2015). *The evolution of popular music: USA 1960–2010*. Royal Society Open Science, 2(5), 150081. https://doi.org/10.1098/rsos.150081

Müller, M., Ewert, S., & Grosche, P. (2010). *Musical structure analysis using audio features*. In Müller, M. (Ed.), Fundamentals of Music Processing (pp. 135–154). Springer. https://doi.org/10.1007/978-3-319-21945-5_6

Pachet, F. (2008). *Hit song science is not yet a science*. Proceedings of the 9th International Conference on Music Information Retrieval (ISMIR), 355–360.

Schedl, M., Gómez, E., & Urbano, J. (2015). *Music information retrieval: Recent developments and applications*. Foundations and Trends in Information Retrieval, 8(2-3), 127–261. https://doi.org/10.1561/1500000042

Serrà, J., Corral, Á., Boguñá, M., Haro, M., & Arcos, J. L. (2012). *Measuring the evolution of contemporary Western popular music*. Scientific Reports, 2(1), 521. https://doi.org/10.1038/srep00521

Spotify. (2023). *How Spotify’s popularity score works*. Spotify Developer Documentation. Retrieved from https://developer.spotify.com/
